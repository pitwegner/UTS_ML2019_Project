{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A2_Project.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pitwegner/UTS_ML2019_Project/blob/channels/A2_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Yzn2si-aysUL"
      },
      "source": [
        "## Notes\n",
        "\n",
        "* All segments have different lengths -> sliding window approach\n",
        "  * maybe length of 100 or 200? I read that the window size matters a lot\n",
        "\n",
        "## Open Questions\n",
        "\n",
        "* Network architecture? -> LSTM, CNN, Hierarchical Attention?\n",
        "  * Maybe implement multiple and compare?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ReiUL--rysUT",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import numpy as np\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g-RL7JyBysUi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "be988ab5-036b-44a7-eb10-b206603b2407"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "activities = pd.read_csv(\"/content/drive/My Drive/train/activities_train.csv\") # Activity Labels for Segments and Nurse ID\n",
        "mocap = pd.DataFrame()\n",
        "print(\"Reading Mocap Data\")\n",
        "i = 0\n",
        "bar_length = 50\n",
        "files = glob.glob(\"/content/drive/My Drive/train/mocap/segment*.csv\")\n",
        "for mf in files:\n",
        "    i += 1\n",
        "    progress = math.ceil(bar_length * i / len(files))\n",
        "    print(\"\\r\", \"[\" + \"=\" * progress + \" \" * (bar_length - progress) + \"] \" + \"{0:.2f}\".format(100 * i / len(files)) + '%', end=\"\")\n",
        "    mocap = mocap.append(pd.read_csv(mf).ffill().bfill().fillna(0))\n",
        "mocap = mocap.reset_index().drop(columns=['index','time_elapsed'])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Reading Mocap Data\n",
            " [==================================================] 100.00%"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eM-losmrzvIu",
        "colab": {}
      },
      "source": [
        "mocap_normalized = (mocap-mocap.min())/(mocap.max()-mocap.min())\n",
        "mocap_normalized.segment_id = mocap.segment_id\n",
        "mocap = mocap_normalized\n",
        "activity_arr = activities.activity_id.unique()\n",
        "activity_arr.sort()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P8N1M0M0W2_K",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "\n",
        "class Dataset(data.Dataset):\n",
        "  \n",
        "  def __init__(self, train, labels):\n",
        "        self.labels = labels\n",
        "        self.data = train\n",
        "\n",
        "  def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        values = self.data[index].drop(columns=['segment_id']).values\n",
        "        if len(values) == 1:\n",
        "            X = np.transpose(values.reshape((29,3)))\n",
        "        else:\n",
        "            X = np.moveaxis(values.reshape((200,29,3)), 2, 0)\n",
        "        sid = self.data[index].segment_id.unique()[0]\n",
        "        labels = self.labels[self.labels.segment_id == sid]\n",
        "        aid = labels.activity_id.values[0]\n",
        "        y = activity_arr.tolist().index(aid)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "dataset = Dataset(mocap, activities)\n",
        "window_length = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y-qq2hNXysVJ",
        "colab": {}
      },
      "source": [
        "class SimpleCNN(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        \n",
        "        self.kernel_size = 3\n",
        "        self.stride = 1\n",
        "        self.padding = 1\n",
        "        self.output_channels = 24\n",
        "        self.hidden_parameters = 64\n",
        "        \n",
        "        self.output_x = int((window_length - self.kernel_size + 2 * self.padding) / self.stride) + 1\n",
        "        self.output_y = int((dataset[0:1][0].shape[1] - self.kernel_size + 2 * self.padding) / self.stride) + 1\n",
        "        \n",
        "        self.conv1 = torch.nn.Conv2d(dataset[0:1][0].shape[0], self.output_channels, kernel_size=self.kernel_size, stride=self.stride, padding=self.padding)\n",
        "        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.fc1 = torch.nn.Linear(self.output_channels * int(self.output_x / 2) * int(self.output_y / 2), self.hidden_parameters)\n",
        "        self.fc2 = torch.nn.Linear(self.hidden_parameters, len(activities.activity_id.unique()))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(-1, self.output_channels * int(self.output_x / 2) * int(self.output_y / 2))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S9yWA08P7wFg",
        "colab": {}
      },
      "source": [
        "from torch.utils.data.sampler import Sampler    \n",
        "\n",
        "class RandomWindowSampler(Sampler):\n",
        "  \n",
        "  def __init__(self, indices):\n",
        "    self.indices = indices\n",
        "  \n",
        "  def __iter__(self):\n",
        "    return (slice(self.indices[i], self.indices[i] + window_length) for i in torch.randperm(len(self.indices)))\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.indices)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KdZrZemGHkB_",
        "colab": {}
      },
      "source": [
        "indices = []\n",
        "for sid in dataset.data.segment_id.unique():\n",
        "    indices += list(dataset.data[dataset.data.segment_id == sid].index[:-window_length])\n",
        "\n",
        "split = int(np.floor(0.15 * len(indices)))\n",
        "np.random.shuffle(indices)\n",
        "train_indices, val_indices, test_indices = indices[split+split:], indices[split:split+split], indices[:split]\n",
        "\n",
        "train_sampler = RandomWindowSampler(train_indices)\n",
        "val_sampler = RandomWindowSampler(val_indices)\n",
        "test_sampler = RandomWindowSampler(test_indices)\n",
        "\n",
        "def get_train_loader(batch_size):\n",
        "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, num_workers=2)\n",
        "    return(train_loader)\n",
        "  \n",
        "val_loader = torch.utils.data.DataLoader(dataset, batch_size=128, sampler=val_sampler, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(dataset, batch_size=4, sampler=test_sampler, num_workers=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zB02w7U8IJK6",
        "colab": {}
      },
      "source": [
        "def createLossAndOptimizer(net, learning_rate=0.001):\n",
        "    \n",
        "    #Loss function\n",
        "    loss = torch.nn.CrossEntropyLoss()\n",
        "    \n",
        "    #Optimizer\n",
        "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "    \n",
        "    return(loss, optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PZvSGvD2I5JT",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "def trainNet(net, batch_size, n_epochs, learning_rate):\n",
        "  \n",
        "    print(\"===== HYPERPARAMETERS =====\")\n",
        "    print(\"batch_size =\", batch_size)\n",
        "    print(\"epochs =\", n_epochs)\n",
        "    print(\"learning_rate =\", learning_rate)\n",
        "    print(\"=\" * 27)\n",
        "    \n",
        "    train_loader = get_train_loader(batch_size)\n",
        "    n_batches = len(train_loader)\n",
        "    \n",
        "    loss, optimizer = createLossAndOptimizer(net, learning_rate)\n",
        "    \n",
        "    training_start_time = time.time()\n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "        \n",
        "        running_loss = 0.0\n",
        "        print_every = 10\n",
        "        start_time = time.time()\n",
        "        \n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            inputs, labels = data\n",
        "            if inputs.shape != (32,3,200,29):\n",
        "                # TODO: Handle leftover batches (<32)\n",
        "                print(inputs, inputs.shape)\n",
        "                continue\n",
        "            inputs, labels = Variable(inputs), Variable(labels)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            #Forward pass, backward pass, optimize\n",
        "            outputs = net(inputs)\n",
        "            loss_size = loss(outputs, labels)\n",
        "            loss_size.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            #Print statistics\n",
        "            running_loss += loss_size.data.item()\n",
        "            \n",
        "            #Print every 10th batch of an epoch and run validation pass\n",
        "            if (i + 1) % (print_every) == 0:\n",
        "                train_losses.append(running_loss / print_every)\n",
        "                print(\"Epoch {}, {:d}% \\t train_loss: {:.2f} took: {:.2f}s\".format(epoch+1, int(100 * (i+1) / len(train_loader)), running_loss / print_every, time.time() - start_time))\n",
        "                running_loss = 0.0\n",
        "            \n",
        "                total_val_loss = 0\n",
        "                for i, data in enumerate(val_loader, 0):\n",
        "\n",
        "                    #Wrap tensors in Variables\n",
        "                    inputs, labels = data\n",
        "                    if inputs.shape != (128,3,200,29):\n",
        "                        # TODO: Handle leftover batches (<128)\n",
        "                        print(inputs, inputs.shape)\n",
        "                        continue\n",
        "                    inputs, labels = Variable(inputs), Variable(labels)\n",
        "                    #Forward pass\n",
        "                    val_outputs = net(inputs)\n",
        "                    val_loss_size = loss(val_outputs, labels)\n",
        "                    total_val_loss += val_loss_size.data.item()\n",
        "\n",
        "                    if i >= print_every:\n",
        "                      break\n",
        "                \n",
        "                val_losses.append(total_val_loss / print_every)\n",
        "                print(\"Validation loss = {:.2f}\".format(total_val_loss / print_every))\n",
        "                start_time = time.time()\n",
        "        \n",
        "    print(\"Training finished, took {:.2f}s\".format(time.time() - training_start_time))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DCwSnqOBJerf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d6a7f636-55c7-492f-9795-c774a5d057fe"
      },
      "source": [
        "CNN = SimpleCNN()\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "trainNet(CNN.double(), batch_size=32, n_epochs=1, learning_rate=0.001)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===== HYPERPARAMETERS =====\n",
            "batch_size = 32\n",
            "epochs = 1\n",
            "learning_rate = 0.001\n",
            "===========================\n",
            "Epoch 1, 0% \t train_loss: 2.04 took: 2.65s\n",
            "Validation loss = 1.82\n",
            "Epoch 1, 0% \t train_loss: 1.65 took: 2.24s\n",
            "Validation loss = 1.83\n",
            "Epoch 1, 0% \t train_loss: 1.57 took: 2.23s\n",
            "Validation loss = 1.64\n",
            "Epoch 1, 0% \t train_loss: 1.49 took: 2.23s\n",
            "Validation loss = 1.61\n",
            "Epoch 1, 0% \t train_loss: 1.45 took: 2.18s\n",
            "Validation loss = 1.60\n",
            "Epoch 1, 0% \t train_loss: 1.58 took: 2.21s\n",
            "Validation loss = 1.58\n",
            "Epoch 1, 0% \t train_loss: 1.36 took: 2.20s\n",
            "Validation loss = 1.57\n",
            "Epoch 1, 0% \t train_loss: 1.33 took: 2.19s\n",
            "Validation loss = 1.49\n",
            "Epoch 1, 0% \t train_loss: 1.39 took: 2.17s\n",
            "Validation loss = 1.48\n",
            "Epoch 1, 0% \t train_loss: 1.35 took: 2.21s\n",
            "Validation loss = 1.52\n",
            "Epoch 1, 0% \t train_loss: 1.32 took: 2.18s\n",
            "Validation loss = 1.48\n",
            "Epoch 1, 0% \t train_loss: 1.31 took: 2.22s\n",
            "Validation loss = 1.47\n",
            "Epoch 1, 0% \t train_loss: 1.24 took: 2.21s\n",
            "Validation loss = 1.39\n",
            "Epoch 1, 0% \t train_loss: 1.24 took: 2.20s\n",
            "Validation loss = 1.34\n",
            "Epoch 1, 0% \t train_loss: 1.21 took: 2.18s\n",
            "Validation loss = 1.33\n",
            "Epoch 1, 0% \t train_loss: 1.22 took: 2.22s\n",
            "Validation loss = 1.37\n",
            "Epoch 1, 0% \t train_loss: 1.19 took: 2.16s\n",
            "Validation loss = 1.31\n",
            "Epoch 1, 0% \t train_loss: 1.20 took: 2.21s\n",
            "Validation loss = 1.33\n",
            "Epoch 1, 0% \t train_loss: 1.18 took: 2.17s\n",
            "Validation loss = 1.32\n",
            "Epoch 1, 0% \t train_loss: 1.19 took: 2.24s\n",
            "Validation loss = 1.22\n",
            "Epoch 1, 0% \t train_loss: 1.15 took: 2.17s\n",
            "Validation loss = 1.26\n",
            "Epoch 1, 0% \t train_loss: 1.13 took: 2.22s\n",
            "Validation loss = 1.26\n",
            "Epoch 1, 0% \t train_loss: 1.08 took: 2.17s\n",
            "Validation loss = 1.24\n",
            "Epoch 1, 0% \t train_loss: 1.06 took: 2.23s\n",
            "Validation loss = 1.23\n",
            "Epoch 1, 0% \t train_loss: 1.08 took: 2.17s\n",
            "Validation loss = 1.29\n",
            "Epoch 1, 0% \t train_loss: 1.13 took: 2.20s\n",
            "Validation loss = 1.13\n",
            "Epoch 1, 0% \t train_loss: 1.09 took: 2.19s\n",
            "Validation loss = 1.10\n",
            "Epoch 1, 0% \t train_loss: 1.03 took: 2.21s\n",
            "Validation loss = 1.18\n",
            "Epoch 1, 0% \t train_loss: 1.00 took: 2.18s\n",
            "Validation loss = 1.10\n",
            "Epoch 1, 0% \t train_loss: 1.06 took: 2.20s\n",
            "Validation loss = 1.22\n",
            "Epoch 1, 0% \t train_loss: 1.17 took: 2.16s\n",
            "Validation loss = 1.15\n",
            "Epoch 1, 0% \t train_loss: 1.10 took: 2.20s\n",
            "Validation loss = 1.17\n",
            "Epoch 1, 0% \t train_loss: 1.06 took: 2.17s\n",
            "Validation loss = 1.11\n",
            "Epoch 1, 1% \t train_loss: 1.07 took: 2.21s\n",
            "Validation loss = 1.11\n",
            "Epoch 1, 1% \t train_loss: 1.04 took: 2.17s\n",
            "Validation loss = 1.07\n",
            "Epoch 1, 1% \t train_loss: 0.95 took: 2.25s\n",
            "Validation loss = 1.05\n",
            "Epoch 1, 1% \t train_loss: 0.95 took: 2.18s\n",
            "Validation loss = 1.02\n",
            "Epoch 1, 1% \t train_loss: 0.99 took: 2.23s\n",
            "Validation loss = 1.01\n",
            "Epoch 1, 1% \t train_loss: 0.91 took: 2.18s\n",
            "Validation loss = 1.02\n",
            "Epoch 1, 1% \t train_loss: 0.99 took: 2.20s\n",
            "Validation loss = 1.00\n",
            "Epoch 1, 1% \t train_loss: 0.91 took: 2.19s\n",
            "Validation loss = 1.02\n",
            "Epoch 1, 1% \t train_loss: 0.94 took: 2.21s\n",
            "Validation loss = 0.96\n",
            "Epoch 1, 1% \t train_loss: 0.95 took: 2.17s\n",
            "Validation loss = 1.03\n",
            "Epoch 1, 1% \t train_loss: 1.07 took: 2.21s\n",
            "Validation loss = 0.99\n",
            "Epoch 1, 1% \t train_loss: 0.86 took: 2.17s\n",
            "Validation loss = 1.12\n",
            "Epoch 1, 1% \t train_loss: 0.89 took: 2.18s\n",
            "Validation loss = 0.90\n",
            "Epoch 1, 1% \t train_loss: 0.95 took: 2.16s\n",
            "Validation loss = 1.05\n",
            "Epoch 1, 1% \t train_loss: 0.94 took: 2.18s\n",
            "Validation loss = 1.05\n",
            "Epoch 1, 1% \t train_loss: 0.92 took: 2.16s\n",
            "Validation loss = 0.93\n",
            "Epoch 1, 1% \t train_loss: 0.87 took: 2.22s\n",
            "Validation loss = 0.91\n",
            "Epoch 1, 1% \t train_loss: 0.79 took: 2.15s\n",
            "Validation loss = 1.00\n",
            "Epoch 1, 1% \t train_loss: 0.90 took: 2.18s\n",
            "Validation loss = 0.95\n",
            "Epoch 1, 1% \t train_loss: 0.91 took: 2.15s\n",
            "Validation loss = 0.90\n",
            "Epoch 1, 1% \t train_loss: 0.88 took: 2.19s\n",
            "Validation loss = 0.95\n",
            "Epoch 1, 1% \t train_loss: 0.89 took: 2.17s\n",
            "Validation loss = 0.96\n",
            "Epoch 1, 1% \t train_loss: 0.88 took: 2.17s\n",
            "Validation loss = 0.86\n",
            "Epoch 1, 1% \t train_loss: 0.79 took: 2.16s\n",
            "Validation loss = 0.82\n",
            "Epoch 1, 1% \t train_loss: 0.82 took: 2.18s\n",
            "Validation loss = 0.88\n",
            "Epoch 1, 1% \t train_loss: 0.85 took: 2.14s\n",
            "Validation loss = 0.90\n",
            "Epoch 1, 1% \t train_loss: 0.80 took: 2.16s\n",
            "Validation loss = 0.89\n",
            "Epoch 1, 1% \t train_loss: 0.76 took: 2.20s\n",
            "Validation loss = 0.93\n",
            "Epoch 1, 1% \t train_loss: 0.76 took: 2.17s\n",
            "Validation loss = 0.86\n",
            "Epoch 1, 1% \t train_loss: 0.73 took: 2.15s\n",
            "Validation loss = 0.84\n",
            "Epoch 1, 1% \t train_loss: 0.75 took: 2.15s\n",
            "Validation loss = 0.86\n",
            "Epoch 1, 1% \t train_loss: 0.74 took: 2.17s\n",
            "Validation loss = 0.82\n",
            "Epoch 1, 1% \t train_loss: 0.71 took: 2.17s\n",
            "Validation loss = 0.87\n",
            "Epoch 1, 2% \t train_loss: 0.82 took: 2.16s\n",
            "Validation loss = 0.88\n",
            "Epoch 1, 2% \t train_loss: 0.78 took: 2.17s\n",
            "Validation loss = 0.82\n",
            "Epoch 1, 2% \t train_loss: 0.77 took: 2.17s\n",
            "Validation loss = 0.86\n",
            "Epoch 1, 2% \t train_loss: 0.79 took: 2.17s\n",
            "Validation loss = 0.84\n",
            "Epoch 1, 2% \t train_loss: 0.73 took: 2.16s\n",
            "Validation loss = 0.94\n",
            "Epoch 1, 2% \t train_loss: 0.73 took: 2.17s\n",
            "Validation loss = 0.80\n",
            "Epoch 1, 2% \t train_loss: 0.72 took: 2.18s\n",
            "Validation loss = 0.85\n",
            "Epoch 1, 2% \t train_loss: 0.71 took: 2.17s\n",
            "Validation loss = 0.88\n",
            "Epoch 1, 2% \t train_loss: 0.84 took: 2.15s\n",
            "Validation loss = 0.83\n",
            "Epoch 1, 2% \t train_loss: 0.69 took: 2.18s\n",
            "Validation loss = 0.75\n",
            "Epoch 1, 2% \t train_loss: 0.73 took: 2.17s\n",
            "Validation loss = 0.76\n",
            "Epoch 1, 2% \t train_loss: 0.73 took: 2.17s\n",
            "Validation loss = 0.78\n",
            "Epoch 1, 2% \t train_loss: 0.68 took: 2.15s\n",
            "Validation loss = 0.75\n",
            "Epoch 1, 2% \t train_loss: 0.61 took: 2.19s\n",
            "Validation loss = 0.83\n",
            "Epoch 1, 2% \t train_loss: 0.69 took: 2.19s\n",
            "Validation loss = 0.74\n",
            "Epoch 1, 2% \t train_loss: 0.72 took: 2.15s\n",
            "Validation loss = 0.77\n",
            "Epoch 1, 2% \t train_loss: 0.83 took: 2.16s\n",
            "Validation loss = 0.81\n",
            "Epoch 1, 2% \t train_loss: 0.67 took: 2.17s\n",
            "Validation loss = 0.77\n",
            "Epoch 1, 2% \t train_loss: 0.72 took: 2.15s\n",
            "Validation loss = 0.78\n",
            "Epoch 1, 2% \t train_loss: 0.71 took: 2.17s\n",
            "Validation loss = 0.74\n",
            "Epoch 1, 2% \t train_loss: 0.59 took: 2.16s\n",
            "Validation loss = 0.72\n",
            "Epoch 1, 2% \t train_loss: 0.76 took: 2.17s\n",
            "Validation loss = 0.76\n",
            "Epoch 1, 2% \t train_loss: 0.67 took: 2.16s\n",
            "Validation loss = 0.75\n",
            "Epoch 1, 2% \t train_loss: 0.70 took: 2.23s\n",
            "Validation loss = 0.78\n",
            "Epoch 1, 2% \t train_loss: 0.71 took: 2.15s\n",
            "Validation loss = 0.78\n",
            "Epoch 1, 2% \t train_loss: 0.66 took: 2.18s\n",
            "Validation loss = 0.71\n",
            "Epoch 1, 2% \t train_loss: 0.60 took: 2.15s\n",
            "Validation loss = 0.69\n",
            "Epoch 1, 2% \t train_loss: 0.73 took: 2.20s\n",
            "Validation loss = 0.69\n",
            "Epoch 1, 2% \t train_loss: 0.67 took: 2.15s\n",
            "Validation loss = 0.68\n",
            "Epoch 1, 2% \t train_loss: 0.66 took: 2.16s\n",
            "Validation loss = 0.67\n",
            "Epoch 1, 2% \t train_loss: 0.66 took: 2.15s\n",
            "Validation loss = 0.78\n",
            "Epoch 1, 2% \t train_loss: 0.61 took: 2.19s\n",
            "Validation loss = 0.67\n",
            "Epoch 1, 2% \t train_loss: 0.58 took: 2.16s\n",
            "Validation loss = 0.77\n",
            "Epoch 1, 3% \t train_loss: 0.64 took: 2.17s\n",
            "Validation loss = 0.70\n",
            "Epoch 1, 3% \t train_loss: 0.64 took: 2.16s\n",
            "Validation loss = 0.65\n",
            "Epoch 1, 3% \t train_loss: 0.71 took: 2.19s\n",
            "Validation loss = 0.73\n",
            "Epoch 1, 3% \t train_loss: 0.67 took: 2.15s\n",
            "Validation loss = 0.65\n",
            "Epoch 1, 3% \t train_loss: 0.62 took: 2.17s\n",
            "Validation loss = 0.66\n",
            "Epoch 1, 3% \t train_loss: 0.68 took: 2.15s\n",
            "Validation loss = 0.71\n",
            "Epoch 1, 3% \t train_loss: 0.60 took: 2.19s\n",
            "Validation loss = 0.63\n",
            "Epoch 1, 3% \t train_loss: 0.62 took: 2.16s\n",
            "Validation loss = 0.66\n",
            "Epoch 1, 3% \t train_loss: 0.64 took: 2.17s\n",
            "Validation loss = 0.68\n",
            "Epoch 1, 3% \t train_loss: 0.60 took: 2.16s\n",
            "Validation loss = 0.68\n",
            "Epoch 1, 3% \t train_loss: 0.63 took: 2.18s\n",
            "Validation loss = 0.62\n",
            "Epoch 1, 3% \t train_loss: 0.69 took: 2.17s\n",
            "Validation loss = 0.69\n",
            "Epoch 1, 3% \t train_loss: 0.57 took: 2.17s\n",
            "Validation loss = 0.61\n",
            "Epoch 1, 3% \t train_loss: 0.56 took: 2.15s\n",
            "Validation loss = 0.70\n",
            "Epoch 1, 3% \t train_loss: 0.57 took: 2.18s\n",
            "Validation loss = 0.65\n",
            "Epoch 1, 3% \t train_loss: 0.58 took: 2.16s\n",
            "Validation loss = 0.66\n",
            "Epoch 1, 3% \t train_loss: 0.59 took: 2.20s\n",
            "Validation loss = 0.61\n",
            "Epoch 1, 3% \t train_loss: 0.59 took: 2.15s\n",
            "Validation loss = 0.63\n",
            "Epoch 1, 3% \t train_loss: 0.66 took: 2.17s\n",
            "Validation loss = 0.62\n",
            "Epoch 1, 3% \t train_loss: 0.59 took: 2.16s\n",
            "Validation loss = 0.58\n",
            "Epoch 1, 3% \t train_loss: 0.49 took: 2.17s\n",
            "Validation loss = 0.66\n",
            "Epoch 1, 3% \t train_loss: 0.63 took: 2.15s\n",
            "Validation loss = 0.58\n",
            "Epoch 1, 3% \t train_loss: 0.52 took: 2.20s\n",
            "Validation loss = 0.63\n",
            "Epoch 1, 3% \t train_loss: 0.60 took: 2.17s\n",
            "Validation loss = 0.66\n",
            "Epoch 1, 3% \t train_loss: 0.50 took: 2.17s\n",
            "Validation loss = 0.61\n",
            "Epoch 1, 3% \t train_loss: 0.55 took: 2.16s\n",
            "Validation loss = 0.58\n",
            "Epoch 1, 3% \t train_loss: 0.43 took: 2.18s\n",
            "Validation loss = 0.62\n",
            "Epoch 1, 3% \t train_loss: 0.55 took: 2.16s\n",
            "Validation loss = 0.58\n",
            "Epoch 1, 3% \t train_loss: 0.63 took: 2.17s\n",
            "Validation loss = 0.65\n",
            "Epoch 1, 3% \t train_loss: 0.55 took: 2.16s\n",
            "Validation loss = 0.59\n",
            "Epoch 1, 3% \t train_loss: 0.56 took: 2.19s\n",
            "Validation loss = 0.62\n",
            "Epoch 1, 3% \t train_loss: 0.58 took: 2.14s\n",
            "Validation loss = 0.64\n",
            "Epoch 1, 3% \t train_loss: 0.55 took: 2.16s\n",
            "Validation loss = 0.60\n",
            "Epoch 1, 3% \t train_loss: 0.53 took: 2.18s\n",
            "Validation loss = 0.55\n",
            "Epoch 1, 4% \t train_loss: 0.51 took: 2.22s\n",
            "Validation loss = 0.58\n",
            "Epoch 1, 4% \t train_loss: 0.52 took: 2.16s\n",
            "Validation loss = 0.58\n",
            "Epoch 1, 4% \t train_loss: 0.49 took: 2.17s\n",
            "Validation loss = 0.53\n",
            "Epoch 1, 4% \t train_loss: 0.52 took: 2.15s\n",
            "Validation loss = 0.56\n",
            "Epoch 1, 4% \t train_loss: 0.53 took: 2.20s\n",
            "Validation loss = 0.66\n",
            "Epoch 1, 4% \t train_loss: 0.53 took: 2.15s\n",
            "Validation loss = 0.52\n",
            "Epoch 1, 4% \t train_loss: 0.50 took: 2.19s\n",
            "Validation loss = 0.56\n",
            "Epoch 1, 4% \t train_loss: 0.54 took: 2.16s\n",
            "Validation loss = 0.56\n",
            "Epoch 1, 4% \t train_loss: 0.57 took: 2.17s\n",
            "Validation loss = 0.55\n",
            "Epoch 1, 4% \t train_loss: 0.46 took: 2.16s\n",
            "Validation loss = 0.58\n",
            "Epoch 1, 4% \t train_loss: 0.62 took: 2.17s\n",
            "Validation loss = 0.55\n",
            "Epoch 1, 4% \t train_loss: 0.57 took: 2.16s\n",
            "Validation loss = 0.53\n",
            "Epoch 1, 4% \t train_loss: 0.47 took: 2.17s\n",
            "Validation loss = 0.55\n",
            "Epoch 1, 4% \t train_loss: 0.50 took: 2.19s\n",
            "Validation loss = 0.58\n",
            "Epoch 1, 4% \t train_loss: 0.55 took: 2.17s\n",
            "Validation loss = 0.64\n",
            "Epoch 1, 4% \t train_loss: 0.61 took: 2.16s\n",
            "Validation loss = 0.51\n",
            "Epoch 1, 4% \t train_loss: 0.53 took: 2.17s\n",
            "Validation loss = 0.57\n",
            "Epoch 1, 4% \t train_loss: 0.58 took: 2.18s\n",
            "Validation loss = 0.51\n",
            "Epoch 1, 4% \t train_loss: 0.47 took: 2.16s\n",
            "Validation loss = 0.56\n",
            "Epoch 1, 4% \t train_loss: 0.53 took: 2.16s\n",
            "Validation loss = 0.55\n",
            "Epoch 1, 4% \t train_loss: 0.50 took: 2.16s\n",
            "Validation loss = 0.55\n",
            "Epoch 1, 4% \t train_loss: 0.52 took: 2.18s\n",
            "Validation loss = 0.54\n",
            "Epoch 1, 4% \t train_loss: 0.54 took: 2.19s\n",
            "Validation loss = 0.56\n",
            "Epoch 1, 4% \t train_loss: 0.48 took: 2.17s\n",
            "Validation loss = 0.52\n",
            "Epoch 1, 4% \t train_loss: 0.52 took: 2.18s\n",
            "Validation loss = 0.56\n",
            "Epoch 1, 4% \t train_loss: 0.45 took: 2.20s\n",
            "Validation loss = 0.55\n",
            "Epoch 1, 4% \t train_loss: 0.46 took: 2.16s\n",
            "Validation loss = 0.59\n",
            "Epoch 1, 4% \t train_loss: 0.55 took: 2.17s\n",
            "Validation loss = 0.51\n",
            "Epoch 1, 4% \t train_loss: 0.46 took: 2.18s\n",
            "Validation loss = 0.53\n",
            "Epoch 1, 4% \t train_loss: 0.43 took: 2.16s\n",
            "Validation loss = 0.53\n",
            "Epoch 1, 4% \t train_loss: 0.46 took: 2.17s\n",
            "Validation loss = 0.54\n",
            "Epoch 1, 4% \t train_loss: 0.57 took: 2.17s\n",
            "Validation loss = 0.48\n",
            "Epoch 1, 4% \t train_loss: 0.49 took: 2.17s\n",
            "Validation loss = 0.51\n",
            "Epoch 1, 5% \t train_loss: 0.52 took: 2.15s\n",
            "Validation loss = 0.54\n",
            "Epoch 1, 5% \t train_loss: 0.46 took: 2.20s\n",
            "Validation loss = 0.48\n",
            "Epoch 1, 5% \t train_loss: 0.54 took: 2.15s\n",
            "Validation loss = 0.51\n",
            "Epoch 1, 5% \t train_loss: 0.50 took: 2.16s\n",
            "Validation loss = 0.51\n",
            "Epoch 1, 5% \t train_loss: 0.48 took: 2.16s\n",
            "Validation loss = 0.52\n",
            "Epoch 1, 5% \t train_loss: 0.52 took: 2.20s\n",
            "Validation loss = 0.49\n",
            "Epoch 1, 5% \t train_loss: 0.48 took: 2.17s\n",
            "Validation loss = 0.66\n",
            "Epoch 1, 5% \t train_loss: 0.46 took: 2.18s\n",
            "Validation loss = 0.62\n",
            "Epoch 1, 5% \t train_loss: 0.43 took: 2.14s\n",
            "Validation loss = 0.49\n",
            "Epoch 1, 5% \t train_loss: 0.45 took: 2.19s\n",
            "Validation loss = 0.53\n",
            "Epoch 1, 5% \t train_loss: 0.48 took: 2.16s\n",
            "Validation loss = 0.55\n",
            "Epoch 1, 5% \t train_loss: 0.55 took: 2.17s\n",
            "Validation loss = 0.55\n",
            "Epoch 1, 5% \t train_loss: 0.48 took: 2.18s\n",
            "Validation loss = 0.57\n",
            "Epoch 1, 5% \t train_loss: 0.51 took: 2.22s\n",
            "Validation loss = 0.55\n",
            "Epoch 1, 5% \t train_loss: 0.40 took: 2.16s\n",
            "Validation loss = 0.47\n",
            "Epoch 1, 5% \t train_loss: 0.45 took: 2.17s\n",
            "Validation loss = 0.51\n",
            "Epoch 1, 5% \t train_loss: 0.46 took: 2.15s\n",
            "Validation loss = 0.47\n",
            "Epoch 1, 5% \t train_loss: 0.38 took: 2.20s\n",
            "Validation loss = 0.49\n",
            "Epoch 1, 5% \t train_loss: 0.44 took: 2.16s\n",
            "Validation loss = 0.42\n",
            "Epoch 1, 5% \t train_loss: 0.49 took: 2.17s\n",
            "Validation loss = 0.46\n",
            "Epoch 1, 5% \t train_loss: 0.43 took: 2.15s\n",
            "Validation loss = 0.48\n",
            "Epoch 1, 5% \t train_loss: 0.39 took: 2.19s\n",
            "Validation loss = 0.46\n",
            "Epoch 1, 5% \t train_loss: 0.46 took: 2.16s\n",
            "Validation loss = 0.42\n",
            "Epoch 1, 5% \t train_loss: 0.44 took: 2.17s\n",
            "Validation loss = 0.50\n",
            "Epoch 1, 5% \t train_loss: 0.35 took: 2.16s\n",
            "Validation loss = 0.45\n",
            "Epoch 1, 5% \t train_loss: 0.49 took: 2.17s\n",
            "Validation loss = 0.51\n",
            "Epoch 1, 5% \t train_loss: 0.50 took: 2.17s\n",
            "Validation loss = 0.44\n",
            "Epoch 1, 5% \t train_loss: 0.39 took: 2.18s\n",
            "Validation loss = 0.44\n",
            "Epoch 1, 5% \t train_loss: 0.43 took: 2.17s\n",
            "Validation loss = 0.52\n",
            "Epoch 1, 5% \t train_loss: 0.43 took: 2.19s\n",
            "Validation loss = 0.44\n",
            "Epoch 1, 5% \t train_loss: 0.50 took: 2.18s\n",
            "Validation loss = 0.50\n",
            "Epoch 1, 5% \t train_loss: 0.37 took: 2.17s\n",
            "Validation loss = 0.48\n",
            "Epoch 1, 5% \t train_loss: 0.47 took: 2.18s\n",
            "Validation loss = 0.46\n",
            "Epoch 1, 6% \t train_loss: 0.43 took: 2.18s\n",
            "Validation loss = 0.55\n",
            "Epoch 1, 6% \t train_loss: 0.50 took: 2.18s\n",
            "Validation loss = 0.44\n",
            "Epoch 1, 6% \t train_loss: 0.46 took: 2.17s\n",
            "Validation loss = 0.50\n",
            "Epoch 1, 6% \t train_loss: 0.42 took: 2.19s\n",
            "Validation loss = 0.50\n",
            "Epoch 1, 6% \t train_loss: 0.46 took: 2.18s\n",
            "Validation loss = 0.47\n",
            "Epoch 1, 6% \t train_loss: 0.37 took: 2.17s\n",
            "Validation loss = 0.50\n",
            "Epoch 1, 6% \t train_loss: 0.30 took: 2.18s\n",
            "Validation loss = 0.46\n",
            "Epoch 1, 6% \t train_loss: 0.34 took: 2.15s\n",
            "Validation loss = 0.45\n",
            "Epoch 1, 6% \t train_loss: 0.40 took: 2.17s\n",
            "Validation loss = 0.49\n",
            "Epoch 1, 6% \t train_loss: 0.44 took: 2.16s\n",
            "Validation loss = 0.48\n",
            "Epoch 1, 6% \t train_loss: 0.46 took: 2.18s\n",
            "Validation loss = 0.53\n",
            "Epoch 1, 6% \t train_loss: 0.45 took: 2.15s\n",
            "Validation loss = 0.49\n",
            "Epoch 1, 6% \t train_loss: 0.34 took: 2.17s\n",
            "Validation loss = 0.48\n",
            "Epoch 1, 6% \t train_loss: 0.42 took: 2.16s\n",
            "Validation loss = 0.45\n",
            "Epoch 1, 6% \t train_loss: 0.43 took: 2.19s\n",
            "Validation loss = 0.40\n",
            "Epoch 1, 6% \t train_loss: 0.42 took: 2.16s\n",
            "Validation loss = 0.50\n",
            "Epoch 1, 6% \t train_loss: 0.41 took: 2.16s\n",
            "Validation loss = 0.46\n",
            "Epoch 1, 6% \t train_loss: 0.42 took: 2.17s\n",
            "Validation loss = 0.42\n",
            "Epoch 1, 6% \t train_loss: 0.45 took: 2.25s\n",
            "Validation loss = 0.49\n",
            "Epoch 1, 6% \t train_loss: 0.44 took: 2.17s\n",
            "Validation loss = 0.43\n",
            "Epoch 1, 6% \t train_loss: 0.41 took: 2.16s\n",
            "Validation loss = 0.43\n",
            "Epoch 1, 6% \t train_loss: 0.46 took: 2.15s\n",
            "Validation loss = 0.41\n",
            "Epoch 1, 6% \t train_loss: 0.36 took: 2.20s\n",
            "Validation loss = 0.39\n",
            "Epoch 1, 6% \t train_loss: 0.41 took: 2.16s\n",
            "Validation loss = 0.49\n",
            "Epoch 1, 6% \t train_loss: 0.45 took: 2.17s\n",
            "Validation loss = 0.40\n",
            "Epoch 1, 6% \t train_loss: 0.38 took: 2.16s\n",
            "Validation loss = 0.41\n",
            "Epoch 1, 6% \t train_loss: 0.47 took: 2.24s\n",
            "Validation loss = 0.44\n",
            "Epoch 1, 6% \t train_loss: 0.33 took: 2.16s\n",
            "Validation loss = 0.42\n",
            "Epoch 1, 6% \t train_loss: 0.31 took: 2.19s\n",
            "Validation loss = 0.41\n",
            "Epoch 1, 6% \t train_loss: 0.37 took: 2.17s\n",
            "Validation loss = 0.40\n",
            "Epoch 1, 6% \t train_loss: 0.45 took: 2.17s\n",
            "Validation loss = 0.40\n",
            "Epoch 1, 6% \t train_loss: 0.38 took: 2.17s\n",
            "Validation loss = 0.41\n",
            "Epoch 1, 6% \t train_loss: 0.36 took: 2.16s\n",
            "Validation loss = 0.42\n",
            "Epoch 1, 7% \t train_loss: 0.36 took: 2.15s\n",
            "Validation loss = 0.46\n",
            "Epoch 1, 7% \t train_loss: 0.39 took: 2.17s\n",
            "Validation loss = 0.47\n",
            "Epoch 1, 7% \t train_loss: 0.36 took: 2.23s\n",
            "Validation loss = 0.43\n",
            "Epoch 1, 7% \t train_loss: 0.31 took: 2.16s\n",
            "Validation loss = 0.43\n",
            "Epoch 1, 7% \t train_loss: 0.43 took: 2.17s\n",
            "Validation loss = 0.39\n",
            "Epoch 1, 7% \t train_loss: 0.39 took: 2.18s\n",
            "Validation loss = 0.47\n",
            "Epoch 1, 7% \t train_loss: 0.33 took: 2.19s\n",
            "Validation loss = 0.40\n",
            "Epoch 1, 7% \t train_loss: 0.40 took: 2.17s\n",
            "Validation loss = 0.52\n",
            "Epoch 1, 7% \t train_loss: 0.36 took: 2.16s\n",
            "Validation loss = 0.46\n",
            "Epoch 1, 7% \t train_loss: 0.37 took: 2.17s\n",
            "Validation loss = 0.42\n",
            "Epoch 1, 7% \t train_loss: 0.41 took: 2.17s\n",
            "Validation loss = 0.41\n",
            "Epoch 1, 7% \t train_loss: 0.41 took: 2.16s\n",
            "Validation loss = 0.36\n",
            "Epoch 1, 7% \t train_loss: 0.37 took: 2.15s\n",
            "Validation loss = 0.44\n",
            "Epoch 1, 7% \t train_loss: 0.36 took: 2.17s\n",
            "Validation loss = 0.42\n",
            "Epoch 1, 7% \t train_loss: 0.35 took: 2.17s\n",
            "Validation loss = 0.39\n",
            "Epoch 1, 7% \t train_loss: 0.39 took: 2.17s\n",
            "Validation loss = 0.37\n",
            "Epoch 1, 7% \t train_loss: 0.33 took: 2.17s\n",
            "Validation loss = 0.39\n",
            "Epoch 1, 7% \t train_loss: 0.36 took: 2.17s\n",
            "Validation loss = 0.38\n",
            "Epoch 1, 7% \t train_loss: 0.38 took: 2.18s\n",
            "Validation loss = 0.42\n",
            "Epoch 1, 7% \t train_loss: 0.36 took: 2.16s\n",
            "Validation loss = 0.45\n",
            "Epoch 1, 7% \t train_loss: 0.41 took: 2.16s\n",
            "Validation loss = 0.41\n",
            "Epoch 1, 7% \t train_loss: 0.41 took: 2.17s\n",
            "Validation loss = 0.45\n",
            "Epoch 1, 7% \t train_loss: 0.41 took: 2.16s\n",
            "Validation loss = 0.39\n",
            "Epoch 1, 7% \t train_loss: 0.40 took: 2.17s\n",
            "Validation loss = 0.46\n",
            "Epoch 1, 7% \t train_loss: 0.39 took: 2.17s\n",
            "Validation loss = 0.37\n",
            "Epoch 1, 7% \t train_loss: 0.31 took: 2.18s\n",
            "Validation loss = 0.41\n",
            "Epoch 1, 7% \t train_loss: 0.35 took: 2.16s\n",
            "Validation loss = 0.38\n",
            "Epoch 1, 7% \t train_loss: 0.33 took: 2.16s\n",
            "Validation loss = 0.47\n",
            "Epoch 1, 7% \t train_loss: 0.30 took: 2.16s\n",
            "Validation loss = 0.38\n",
            "Epoch 1, 7% \t train_loss: 0.34 took: 2.15s\n",
            "Validation loss = 0.42\n",
            "Epoch 1, 7% \t train_loss: 0.39 took: 2.16s\n",
            "Validation loss = 0.36\n",
            "Epoch 1, 7% \t train_loss: 0.31 took: 2.17s\n",
            "Validation loss = 0.39\n",
            "Epoch 1, 7% \t train_loss: 0.34 took: 2.16s\n",
            "Validation loss = 0.40\n",
            "Epoch 1, 7% \t train_loss: 0.32 took: 2.17s\n",
            "Validation loss = 0.34\n",
            "Epoch 1, 8% \t train_loss: 0.34 took: 2.15s\n",
            "Validation loss = 0.36\n",
            "Epoch 1, 8% \t train_loss: 0.43 took: 2.17s\n",
            "Validation loss = 0.49\n",
            "Epoch 1, 8% \t train_loss: 0.37 took: 2.16s\n",
            "Validation loss = 0.41\n",
            "Epoch 1, 8% \t train_loss: 0.37 took: 2.17s\n",
            "Validation loss = 0.44\n",
            "Epoch 1, 8% \t train_loss: 0.41 took: 2.16s\n",
            "Validation loss = 0.40\n",
            "Epoch 1, 8% \t train_loss: 0.31 took: 2.18s\n",
            "Validation loss = 0.39\n",
            "Epoch 1, 8% \t train_loss: 0.31 took: 2.16s\n",
            "Validation loss = 0.39\n",
            "Epoch 1, 8% \t train_loss: 0.36 took: 2.18s\n",
            "Validation loss = 0.38\n",
            "Epoch 1, 8% \t train_loss: 0.30 took: 2.17s\n",
            "Validation loss = 0.34\n",
            "Epoch 1, 8% \t train_loss: 0.40 took: 2.18s\n",
            "Validation loss = 0.37\n",
            "Epoch 1, 8% \t train_loss: 0.42 took: 2.15s\n",
            "Validation loss = 0.38\n",
            "Epoch 1, 8% \t train_loss: 0.39 took: 2.17s\n",
            "Validation loss = 0.43\n",
            "Epoch 1, 8% \t train_loss: 0.42 took: 2.15s\n",
            "Validation loss = 0.39\n",
            "Epoch 1, 8% \t train_loss: 0.32 took: 2.18s\n",
            "Validation loss = 0.38\n",
            "Epoch 1, 8% \t train_loss: 0.40 took: 2.15s\n",
            "Validation loss = 0.42\n",
            "Epoch 1, 8% \t train_loss: 0.43 took: 2.16s\n",
            "Validation loss = 0.45\n",
            "Epoch 1, 8% \t train_loss: 0.40 took: 2.16s\n",
            "Validation loss = 0.48\n",
            "Epoch 1, 8% \t train_loss: 0.46 took: 2.17s\n",
            "Validation loss = 0.51\n",
            "Epoch 1, 8% \t train_loss: 0.44 took: 2.15s\n",
            "Validation loss = 0.44\n",
            "Epoch 1, 8% \t train_loss: 0.38 took: 2.16s\n",
            "Validation loss = 0.39\n",
            "Epoch 1, 8% \t train_loss: 0.37 took: 2.18s\n",
            "Validation loss = 0.37\n",
            "Epoch 1, 8% \t train_loss: 0.37 took: 2.16s\n",
            "Validation loss = 0.43\n",
            "Epoch 1, 8% \t train_loss: 0.43 took: 2.16s\n",
            "Validation loss = 0.40\n",
            "Epoch 1, 8% \t train_loss: 0.29 took: 2.16s\n",
            "Validation loss = 0.36\n",
            "Epoch 1, 8% \t train_loss: 0.40 took: 2.17s\n",
            "Validation loss = 0.43\n",
            "Epoch 1, 8% \t train_loss: 0.35 took: 2.17s\n",
            "Validation loss = 0.42\n",
            "Epoch 1, 8% \t train_loss: 0.32 took: 2.17s\n",
            "Validation loss = 0.37\n",
            "Epoch 1, 8% \t train_loss: 0.39 took: 2.17s\n",
            "Validation loss = 0.36\n",
            "Epoch 1, 8% \t train_loss: 0.33 took: 2.19s\n",
            "Validation loss = 0.39\n",
            "Epoch 1, 8% \t train_loss: 0.34 took: 2.18s\n",
            "Validation loss = 0.37\n",
            "Epoch 1, 8% \t train_loss: 0.37 took: 2.16s\n",
            "Validation loss = 0.39\n",
            "Epoch 1, 8% \t train_loss: 0.32 took: 2.16s\n",
            "Validation loss = 0.42\n",
            "Epoch 1, 8% \t train_loss: 0.36 took: 2.18s\n",
            "Validation loss = 0.36\n",
            "Epoch 1, 9% \t train_loss: 0.30 took: 2.18s\n",
            "Validation loss = 0.35\n",
            "Epoch 1, 9% \t train_loss: 0.34 took: 2.16s\n",
            "Validation loss = 0.39\n",
            "Epoch 1, 9% \t train_loss: 0.28 took: 2.17s\n",
            "Validation loss = 0.36\n",
            "Epoch 1, 9% \t train_loss: 0.34 took: 2.18s\n",
            "Validation loss = 0.39\n",
            "Epoch 1, 9% \t train_loss: 0.32 took: 2.18s\n",
            "Validation loss = 0.35\n",
            "Epoch 1, 9% \t train_loss: 0.35 took: 2.15s\n",
            "Validation loss = 0.33\n",
            "Epoch 1, 9% \t train_loss: 0.29 took: 2.16s\n",
            "Validation loss = 0.41\n",
            "Epoch 1, 9% \t train_loss: 0.40 took: 2.18s\n",
            "Validation loss = 0.40\n",
            "Epoch 1, 9% \t train_loss: 0.40 took: 2.16s\n",
            "Validation loss = 0.36\n",
            "Epoch 1, 9% \t train_loss: 0.29 took: 2.16s\n",
            "Validation loss = 0.39\n",
            "Epoch 1, 9% \t train_loss: 0.33 took: 2.17s\n",
            "Validation loss = 0.42\n",
            "Epoch 1, 9% \t train_loss: 0.32 took: 2.17s\n",
            "Validation loss = 0.34\n",
            "Epoch 1, 9% \t train_loss: 0.39 took: 2.16s\n",
            "Validation loss = 0.43\n",
            "Epoch 1, 9% \t train_loss: 0.36 took: 2.15s\n",
            "Validation loss = 0.37\n",
            "Epoch 1, 9% \t train_loss: 0.38 took: 2.18s\n",
            "Validation loss = 0.39\n",
            "Epoch 1, 9% \t train_loss: 0.35 took: 2.15s\n",
            "Validation loss = 0.32\n",
            "Epoch 1, 9% \t train_loss: 0.32 took: 2.18s\n",
            "Validation loss = 0.33\n",
            "Epoch 1, 9% \t train_loss: 0.30 took: 2.15s\n",
            "Validation loss = 0.34\n",
            "Epoch 1, 9% \t train_loss: 0.34 took: 2.16s\n",
            "Validation loss = 0.36\n",
            "Epoch 1, 9% \t train_loss: 0.28 took: 2.15s\n",
            "Validation loss = 0.34\n",
            "Epoch 1, 9% \t train_loss: 0.34 took: 2.19s\n",
            "Validation loss = 0.31\n",
            "Epoch 1, 9% \t train_loss: 0.41 took: 2.16s\n",
            "Validation loss = 0.42\n",
            "Epoch 1, 9% \t train_loss: 0.40 took: 2.17s\n",
            "Validation loss = 0.45\n",
            "Epoch 1, 9% \t train_loss: 0.34 took: 2.15s\n",
            "Validation loss = 0.35\n",
            "Epoch 1, 9% \t train_loss: 0.28 took: 2.19s\n",
            "Validation loss = 0.35\n",
            "Epoch 1, 9% \t train_loss: 0.30 took: 2.16s\n",
            "Validation loss = 0.34\n",
            "Epoch 1, 9% \t train_loss: 0.31 took: 2.18s\n",
            "Validation loss = 0.42\n",
            "Epoch 1, 9% \t train_loss: 0.41 took: 2.16s\n",
            "Validation loss = 0.41\n",
            "Epoch 1, 9% \t train_loss: 0.39 took: 2.20s\n",
            "Validation loss = 0.40\n",
            "Epoch 1, 9% \t train_loss: 0.30 took: 2.16s\n",
            "Validation loss = 0.37\n",
            "Epoch 1, 9% \t train_loss: 0.28 took: 2.15s\n",
            "Validation loss = 0.36\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-16cec0c59c04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mval_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrainNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-250ee7ccc711>\u001b[0m in \u001b[0;36mtrainNet\u001b[0;34m(net, batch_size, n_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;31m#Forward pass, backward pass, optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mloss_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mloss_size\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-db909778a7a8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_channels\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_x\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_y\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/pooling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    144\u001b[0m         return F.max_pool2d(input, self.kernel_size, self.stride,\n\u001b[1;32m    145\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                             self.return_indices)\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m     return torch.max_pool2d(\n\u001b[0;32m--> 494\u001b[0;31m         input, kernel_size, stride, padding, dilation, ceil_mode)\n\u001b[0m\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m max_pool2d = torch._jit_internal.boolean_dispatch(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BuiOtzV1Qh7R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "e2df7ffd-b112-4bbb-fb7d-c3bbc1b14049"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(x=np.arange(len(train_losses)), y=train_losses, mode='lines', name='train_loss'))\n",
        "fig.add_trace(go.Scatter(x=np.arange(len(val_losses)), y=val_losses, mode='lines', name='val_loss'))\n",
        "fig.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"7ddc0faf-e777-4a86-969a-72a40d8c38d2\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"7ddc0faf-e777-4a86-969a-72a40d8c38d2\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '7ddc0faf-e777-4a86-969a-72a40d8c38d2',\n",
              "                        [{\"mode\": \"lines\", \"name\": \"train_loss\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329], \"y\": [2.043927660906833, 1.6518277767568708, 1.570648835910768, 1.4903968749189187, 1.4516915974703952, 1.5755493064366888, 1.3582272057812461, 1.3324651095753022, 1.3881124951781134, 1.3456376596405404, 1.3197245734615763, 1.3143819414110112, 1.2392807872306337, 1.236186564697285, 1.2116661593145022, 1.21998830607949, 1.1882324207159645, 1.1987171439490594, 1.1763573652818058, 1.1871459333845773, 1.1531103638519788, 1.127035877077716, 1.0778909351691266, 1.0601519957840557, 1.0812648604126882, 1.1304091889452708, 1.0914885352318195, 1.032775284269956, 0.9951008906030328, 1.0561559231578994, 1.1660897886141028, 1.0961757924943174, 1.0616707048956524, 1.0674609834240358, 1.040087516265411, 0.948827165865955, 0.9546515928617378, 0.9850485073904929, 0.9052195386259653, 0.9940163588358522, 0.9143878229319456, 0.9383607139251444, 0.9491059275017986, 1.0669820756545119, 0.8553347966065588, 0.890660882614732, 0.953261801385257, 0.9376098106759321, 0.9237720815836005, 0.8744816313827599, 0.7918907509775931, 0.8969887248022724, 0.9063840820807736, 0.8847944426323651, 0.8892684234407954, 0.8838890164280464, 0.7858233411334015, 0.8178527800094612, 0.847936494642678, 0.7994013263922922, 0.7640196467360963, 0.7616934071341654, 0.7264564858481926, 0.7468558757336943, 0.7402634871982636, 0.7091610581218382, 0.8212866746125915, 0.7775871851877197, 0.7691109305148144, 0.7903295530224256, 0.7263995221153265, 0.7259068614433912, 0.7231834693437772, 0.7085177100967636, 0.8364970667613498, 0.6935652918358096, 0.7316588971128308, 0.7343925694795003, 0.6818943314952559, 0.6129051866511741, 0.6868330891498979, 0.7215194028736056, 0.8253168079167136, 0.6662746517427112, 0.7170946166163376, 0.7060013057457593, 0.585900419244387, 0.7585698494169155, 0.6662515508393929, 0.7013482517608646, 0.7072644059050482, 0.6583656468185036, 0.5964298108952303, 0.7347624694632789, 0.668023096810066, 0.6623685513881697, 0.65733392036961, 0.6060308833782793, 0.5849456000210992, 0.6368982583567055, 0.6420253686041384, 0.7079411481413699, 0.6716567701732952, 0.6197482875026424, 0.6791831609898143, 0.603576334417588, 0.6176486329443803, 0.6421373498166455, 0.6027698780932574, 0.6343698194287992, 0.6911062237450258, 0.5745973968607493, 0.5552602746950874, 0.5721327979138949, 0.5770049281247027, 0.5879944243040868, 0.5856608565966352, 0.6584223136730911, 0.5896242073987018, 0.4948250786453192, 0.6337931546794923, 0.523404906945668, 0.5970264709108233, 0.5035688894295234, 0.5517907592072999, 0.4297796203751929, 0.545059511704703, 0.6329033236709771, 0.5450968489758107, 0.5562296286442859, 0.578872382463192, 0.552492032674642, 0.5279393674530287, 0.5063419787871963, 0.52015432445309, 0.48852068791097025, 0.5161836472398442, 0.5312189499871915, 0.527408547097954, 0.49811554693924426, 0.5353808727633753, 0.5697589312757707, 0.4557938734465047, 0.616971306901079, 0.5654080463992888, 0.4716631100978157, 0.4970625459843568, 0.5484278701174571, 0.6110339500032398, 0.5332113118794409, 0.5833270134778925, 0.47426007088076394, 0.5282294784595385, 0.4971656827534582, 0.5226939352224793, 0.5436997872457644, 0.48444076865427804, 0.5214442086720734, 0.4481447902768914, 0.4558133587134724, 0.5511532889033921, 0.4646385929823185, 0.4331170000110672, 0.4574311882889453, 0.5650035080991275, 0.49170775371732145, 0.5157416110295376, 0.4617585827473675, 0.5381809808154946, 0.5010836942063348, 0.4832290893178898, 0.5213468286153173, 0.482071609226648, 0.45683189213182585, 0.4322024211258294, 0.4536819371985933, 0.48258427737338006, 0.5541701289330926, 0.48082987621085715, 0.5061090274880315, 0.40076618871115544, 0.4547717602114358, 0.45696848553471237, 0.3805643444401292, 0.44359359134687565, 0.49173085655276416, 0.43365821066606036, 0.3938161407086125, 0.4623209849156469, 0.4362189192605099, 0.3534477605159972, 0.4944382716165965, 0.5031512500022464, 0.39303855413417066, 0.42962467445538344, 0.4335251104073527, 0.5017337916499636, 0.36812288480834066, 0.4656592425031601, 0.43115254181266743, 0.4985110743809115, 0.4610640276337602, 0.4172265019784672, 0.4613402257398954, 0.365799752552453, 0.30469580997547613, 0.3424441608444518, 0.39758290364682247, 0.43587782050266943, 0.4640788592156392, 0.445690083415285, 0.3411105839027891, 0.42188770400795195, 0.4312543968422194, 0.4240157902001672, 0.41042103860438084, 0.41951215101358325, 0.45343030485929414, 0.43845359145490537, 0.41429952939224285, 0.4555821087441574, 0.36487731607864826, 0.4131570417729189, 0.4458260956215844, 0.37602996220456375, 0.46604254975937287, 0.3330506293265583, 0.30831245610227215, 0.3735817586172257, 0.44755404319895964, 0.3808873708516101, 0.3611556163495712, 0.3552237304792415, 0.3882583832602903, 0.35523167760105007, 0.30616320487064663, 0.42903623224425613, 0.3937266816053332, 0.32988462144424274, 0.40497526401097594, 0.35952162090744394, 0.366579039892294, 0.4119733755504316, 0.4096235845469608, 0.36957535991772184, 0.35758773880768013, 0.3473556264312684, 0.38977539856592086, 0.32997991171251895, 0.3580382428062264, 0.38425085301498185, 0.36113197083567283, 0.40638589689011917, 0.40878755260749655, 0.4089716078232458, 0.3985321697917345, 0.386881882780772, 0.31137099383670047, 0.3540282076946107, 0.3343615506355172, 0.29590984609562204, 0.3439738517255851, 0.3903482306494312, 0.3139338055814408, 0.3363778185337489, 0.3156957759495399, 0.3358405380006091, 0.43106714212026087, 0.36878485527967053, 0.3719907479610303, 0.4057216491841319, 0.3085931364335428, 0.31380560007303615, 0.35724194216113714, 0.29877193472267827, 0.4015438625127324, 0.4241689832390483, 0.3948925767606067, 0.4185117301957179, 0.3243143062450019, 0.4045179222137027, 0.42529421641028636, 0.39814265256669523, 0.4562720690451524, 0.43962591053667566, 0.38102812637916444, 0.3746528982540801, 0.3738563659102826, 0.4287678903738524, 0.2920450686493566, 0.40216211272903546, 0.3504245787160809, 0.32131986067470264, 0.3870708866053738, 0.3251637769281223, 0.3377920975516029, 0.37229906154136866, 0.32247651851376447, 0.36120120716385024, 0.2985913731106761, 0.3414723736538841, 0.275266134160495, 0.33583156601964864, 0.3179735839590604, 0.34899624520810696, 0.29198559751832537, 0.40381207307284245, 0.3950133938136618, 0.2854615984011773, 0.3304133509862369, 0.3237193662322556, 0.38872664596831485, 0.36249548652674896, 0.3789936364225679, 0.3512044178117272, 0.3164851067481072, 0.29944937498952306, 0.3397678926953001, 0.2841243253312166, 0.34419723191493123, 0.408989844451397, 0.3963124217339186, 0.34055709723929223, 0.2825510650144222, 0.30407438536622833, 0.30999029798768524, 0.40969704218999714, 0.3878833042447182, 0.2987433678520889, 0.2832710400485793]}, {\"mode\": \"lines\", \"name\": \"val_loss\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329], \"y\": [1.823647835989301, 1.8287887372526357, 1.6429775284592225, 1.6122007045727478, 1.5999069370965795, 1.5828405328942199, 1.5676651222137785, 1.491065259483354, 1.4768591325438414, 1.5160228769770545, 1.4792335552230502, 1.4747785012860055, 1.3859013097056487, 1.3353445188059896, 1.3310480515023506, 1.371004875578921, 1.310558853053713, 1.3273229739893906, 1.324238444327972, 1.2198642752725282, 1.263139611409799, 1.2608513044841456, 1.2393851663445044, 1.2259105708828972, 1.2850816378721737, 1.130751503334865, 1.0983427899133258, 1.1846320389630445, 1.1044574598160242, 1.2239410845958565, 1.1476670157237305, 1.1668862872883579, 1.1064078265585304, 1.1094808698183523, 1.0654736383524845, 1.0481539572368774, 1.024265600834254, 1.0146196178450189, 1.021132460632131, 1.002066357272858, 1.0249178061459214, 0.9577459966782357, 1.0273719788178017, 0.9877965928682547, 1.1228540434920493, 0.9028815088423217, 1.0530547065507905, 1.0508733761386668, 0.9307354933646594, 0.9056245598945081, 1.0025224817870726, 0.9548072930903967, 0.9030369180279415, 0.9489843577407509, 0.9619447334723971, 0.8595052296706045, 0.82136607401492, 0.876515505638106, 0.9019808070841115, 0.8863181603851533, 0.9293626168464761, 0.8588268989122072, 0.8389872790003299, 0.8624147672004605, 0.8202864634413685, 0.8692256077994894, 0.8771393236790054, 0.8153244377633658, 0.8587686997868731, 0.8419935231249548, 0.9365968923684255, 0.802584614574586, 0.8516695645021148, 0.8818830048443862, 0.8296480557371056, 0.7457452526306266, 0.7616704466591719, 0.7814883146195285, 0.7467305890425358, 0.8257988250535508, 0.7386688219614538, 0.7710199115507482, 0.8061048438215664, 0.7721225050208289, 0.7767095227124997, 0.7415039097855676, 0.7179359891809524, 0.7584834251753202, 0.7458091770639593, 0.7777798426206186, 0.7764843825019614, 0.7070533817464374, 0.6919940850407766, 0.6853528461699323, 0.6808220535967643, 0.6695174835140751, 0.7787875596282566, 0.6662445430336057, 0.7662955969247212, 0.7000283134774447, 0.6452198394969517, 0.726329953430563, 0.6514066737007008, 0.6575293292609752, 0.7083253565787727, 0.6255711116902221, 0.656402815845642, 0.6772487907034659, 0.682434672530555, 0.6152281026383425, 0.6937907961164287, 0.6144980524173214, 0.6958892236821321, 0.6500332582230554, 0.6582912501855487, 0.6091795421191508, 0.6286316809141166, 0.6248246909485001, 0.5775921047997683, 0.6597943729908681, 0.5832184738557239, 0.6262488356898026, 0.663840892490035, 0.606787385212759, 0.5836445691463712, 0.6208510660845985, 0.5835625663087773, 0.6460853419468096, 0.5896378528527771, 0.6181280428641049, 0.6427850947293324, 0.5972283011785202, 0.5459102780953324, 0.5779412821941938, 0.5784032173458451, 0.5284002025285345, 0.5574613016336248, 0.6646458026707562, 0.5194960718539854, 0.5636316340338141, 0.5621134098264065, 0.5494352727832152, 0.5835034920731246, 0.5464888960766713, 0.5295732366832961, 0.5518917602186821, 0.5775926105434208, 0.640222176147053, 0.5140803752164379, 0.5694625904816907, 0.5132401785094098, 0.55755456482011, 0.5523418044326454, 0.5540092739338468, 0.5417118252851292, 0.5615698929648799, 0.5219457627434754, 0.564148600883022, 0.5455951326217753, 0.5920396767607812, 0.5110574333375048, 0.5259518104606326, 0.5280060563577459, 0.5386121684951527, 0.4832553002976576, 0.5128795665805704, 0.5357210568815458, 0.4796880686629468, 0.5069158939884891, 0.5066963801630714, 0.5150753758688793, 0.4931118085909299, 0.6627151512894993, 0.6191283846616586, 0.4948226217486405, 0.5345488560073074, 0.5450647110104097, 0.5453646229957267, 0.5718719596996985, 0.5471378645993231, 0.470281596132725, 0.5111455236231833, 0.4724685450386853, 0.48566091001155137, 0.42427195349528557, 0.45868052775682466, 0.481966489151713, 0.4564738912401869, 0.41590537541209976, 0.4968379377453675, 0.45493706671806133, 0.5127032824583436, 0.4410935824391169, 0.4421129107474049, 0.5216388816038541, 0.4441050022687126, 0.4993818559474472, 0.4825174866996152, 0.4558688689177345, 0.5465922934888625, 0.44112411135484575, 0.5026249340117219, 0.5011928655606074, 0.4749945245608515, 0.49516910432654715, 0.46239373487513913, 0.449041246248424, 0.48866719870893427, 0.47551464794003506, 0.5322171349344914, 0.49421372843517275, 0.4835081564985414, 0.44540046160695257, 0.39639753489856844, 0.5046271923581298, 0.46129647316174954, 0.41580276726815235, 0.4882103820810091, 0.4299503185312603, 0.43225350007726726, 0.41024199681794266, 0.39140991317259066, 0.49369359780396166, 0.3974122776810944, 0.4119776266171307, 0.4426585123731871, 0.4214344374254259, 0.40772172261324335, 0.40120279990459995, 0.40411477563447373, 0.413094362840142, 0.4224173674258572, 0.46042278302112793, 0.4709077379002659, 0.43377836763100924, 0.4341402150352547, 0.3938628183895546, 0.4723715163469938, 0.39726434357936635, 0.5154868120300515, 0.4554840888635877, 0.4197015639594346, 0.40758173219029253, 0.3647018331730786, 0.438881139366901, 0.4177485620473177, 0.38701538249061745, 0.3729576441889294, 0.3864712607630817, 0.38087539788272473, 0.4213932759464402, 0.4500440844462557, 0.40719200905722586, 0.4508321103711335, 0.39207834498341587, 0.4609620246056679, 0.3695624405367859, 0.4131990141955807, 0.3824202995255854, 0.4737876883768128, 0.37592528954297755, 0.4196243376971539, 0.35708158364175435, 0.3924018005326171, 0.4044130354930126, 0.33525536063568173, 0.358559600726529, 0.4921730120502564, 0.40794694305707574, 0.43705715240524584, 0.4020500672821471, 0.3926841800861448, 0.39457147647664303, 0.37682915209048184, 0.3423220099304238, 0.3729297168358305, 0.38150798493372357, 0.43282398926116594, 0.39254183076985083, 0.3758921059320245, 0.422622231139138, 0.4459029051322399, 0.48331488202566986, 0.511348046314245, 0.44131902367621023, 0.38964408891531976, 0.3703105870590969, 0.4313197284543147, 0.397550230217372, 0.357558353065391, 0.42516021646098034, 0.4202916240766271, 0.36517306866318516, 0.35730758972754867, 0.38566975000796067, 0.3734060819155434, 0.3928192849176377, 0.4212731368956198, 0.3591204579369464, 0.3528453005140365, 0.3949027946232472, 0.36370306189715934, 0.39358263598725596, 0.34975775344729865, 0.3329654089465273, 0.41110364779035286, 0.3976706379327594, 0.3551270047104765, 0.38907856777015104, 0.41508871581128404, 0.3415788931125604, 0.4279917493781178, 0.3702906148586279, 0.39062109498199604, 0.3158732012852527, 0.33341124284679957, 0.3402393254259387, 0.36429051630582, 0.34160096209353297, 0.30780364471424504, 0.42136789363388294, 0.4512457115814962, 0.35336270740272274, 0.3497527061028055, 0.3436243736871669, 0.42044811628295664, 0.4074087963836936, 0.39943967890395327, 0.37381652870167403, 0.36159736155664696]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('7ddc0faf-e777-4a86-969a-72a40d8c38d2');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cigZzw7XcGy-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9b466f07-c4f2-444e-ec54-2b7cde15ce5b"
      },
      "source": [
        "np.set_printoptions(suppress=True)\n",
        "confusion_matrix = np.zeros((6,6))\n",
        "print(\"Starting Test Run\")\n",
        "for i, data in enumerate(test_loader, 0):\n",
        "\n",
        "    #Wrap tensors in Variables\n",
        "    inputs, labels = data\n",
        "    if inputs.shape != (4,3,200,29):\n",
        "        # TODO: Handle leftover batches (<4)\n",
        "        print(inputs, inputs.shape)\n",
        "        continue\n",
        "    inputs, labels = Variable(inputs), Variable(labels)\n",
        "    #Forward pass\n",
        "    val_outputs = CNN.double()(inputs)\n",
        "    value, index = val_outputs[0].max(0)\n",
        "    confusion_matrix[index.item(), labels[0].item()] += 1\n",
        "    print(\"\\r\", \"{0:.2f}%\".format(100 * i / len(test_loader)), end=\"\")\n",
        "print(confusion_matrix)\n",
        "print(\"{0:.2f}%\".format(100 * np.trace(confusion_matrix)/np.sum(confusion_matrix)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Test Run\n",
            " 15.02%"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x62HKQQn-7P3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}